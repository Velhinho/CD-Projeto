{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import matplotlib.colors as clrs\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from imblearn.over_sampling import SMOTE\n",
    "fp_ds1 = \"./DS2_normalized_not_balanced.csv\"\n",
    "\n",
    "original = pd.read_csv(fp_ds1, parse_dates=False, sep=',',index_col=0)\n",
    "\n",
    "my_palette = {'yellow': '#ECD474', 'pale orange': '#E9AE4E', 'salmon': '#E2A36B', 'orange': '#F79522', 'dark orange': '#D7725E',\n",
    "              'pale acqua': '#92C4AF', 'acqua': '#64B29E', 'marine': '#3D9EA9', 'green': '#10A48A', 'olive': '#99C244',\n",
    "              'pale blue': '#BDDDE0', 'blue2': '#199ED5', 'blue3': '#1DAFE5', 'dark blue': '#0C70B2',\n",
    "              'pale pink': '#D077AC', 'pink': '#EA4799', 'lavender': '#E09FD5', 'lilac': '#B081B9', 'purple': '#923E97',\n",
    "              'white': '#FFFFFF', 'light grey': '#D2D3D4', 'grey': '#939598', 'black': '#000000'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "NR_COLUMNS: int = 3\n",
    "HEIGHT: int = 4\n",
    "\n",
    "\n",
    "def choose_grid(nr):\n",
    "    if nr < NR_COLUMNS:\n",
    "        return 1, nr\n",
    "    else:\n",
    "        return (nr // NR_COLUMNS, NR_COLUMNS) if nr % NR_COLUMNS == 0 else (nr // NR_COLUMNS + 1, NR_COLUMNS)\n",
    "\n",
    "\n",
    "def set_axes(xvalues: list, ax: plt.Axes = None, title: str = '', xlabel: str = '', ylabel: str = '', percentage=False):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    if percentage:\n",
    "        ax.set_ylim(0.0, 1.0)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xticklabels(xvalues, fontsize='small', ha='center')\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def set_locators(xvalues: list, ax: plt.Axes = None):\n",
    "    if isinstance(xvalues[0], dt.datetime):\n",
    "        locator = mdates.AutoDateLocator()\n",
    "        ax.xaxis.set_major_locator(locator)\n",
    "        ax.xaxis.set_major_formatter(mdates.AutoDateFormatter(locator, defaultfmt='%Y-%m-%d'))\n",
    "    else:\n",
    "        ax.set_xticks(xvalues)\n",
    "        ax.set_xlim(xvalues[0], xvalues[-1])\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_line(xvalues: list, yvalues: list, ax: plt.Axes = None, title: str = '', xlabel: str = '',\n",
    "              ylabel: str = '', percentage=False):\n",
    "    ax = set_axes(xvalues, ax=ax, title=title, xlabel=xlabel, ylabel=ylabel, percentage=percentage)\n",
    "    ax = set_locators(xvalues, ax=ax)\n",
    "    ax.plot(xvalues,  yvalues, c=cfg.LINE_COLOR)\n",
    "\n",
    "\n",
    "def multiple_line_chart(xvalues: list, yvalues: dict, ax: plt.Axes = None, title: str = '',\n",
    "                        xlabel: str = '', ylabel: str = '', percentage=False, miny=0, maxy=1):\n",
    "    ax = set_axes(xvalues, ax=ax, title=title, xlabel=xlabel, ylabel=ylabel, percentage=percentage)\n",
    "    ax = set_locators(xvalues, ax=ax)\n",
    "\n",
    "    legend: list = []\n",
    "    if percentage:\n",
    "        ax.set_ylim(miny-0.1,maxy+0.1)\n",
    "    for name, y in yvalues.items():\n",
    "        ax.plot(xvalues, y)\n",
    "        legend.append(name)\n",
    "    ax.legend(legend)\n",
    "\n",
    "\n",
    "def bar_chart(xvalues: list, yvalues: list, ax: plt.Axes = None, title: str = '',\n",
    "              xlabel: str = '', ylabel: str = '', percentage=False):\n",
    "    ax = set_axes(xvalues, ax=ax, title=title, xlabel=xlabel, ylabel=ylabel, percentage=percentage)\n",
    "    ax.bar(xvalues, yvalues, edgecolor=my_palette['dark blue'], color=my_palette['pale blue'])\n",
    "\n",
    "\n",
    "def multiple_bar_chart(xvalues: list, yvalues: dict, ax: plt.Axes = None, title: str = '',\n",
    "                       xlabel: str = '', ylabel: str = '', percentage=False):\n",
    "    ax = set_axes(xvalues, ax=ax, title=title, xlabel=xlabel, ylabel=ylabel, percentage=percentage)\n",
    "\n",
    "    x = np.arange(len(xvalues))  # the label locations\n",
    "\n",
    "    width = 0.8 / (len(xvalues)*len(yvalues))\n",
    "    # the width of the bars\n",
    "    step = width / len(xvalues)\n",
    "    i: int = 0\n",
    "    for metric in yvalues:\n",
    "        ax.bar(x + i*width, yvalues[metric], width=width, align='center', label=metric)\n",
    "        i += 1\n",
    "    ax.set_xticks(x + width/len(xvalues) - step/2)\n",
    "    ax.legend(fontsize='x-small', title_fontsize='small')\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cnf_matrix: np.ndarray, classes_names: np.ndarray,\n",
    "                          ax: plt.Axes = None, normalize: bool = False):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    if normalize:\n",
    "        total = cnf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "        cm = cnf_matrix.astype('float') / total\n",
    "        title = \"Normalized confusion matrix\"\n",
    "    else:\n",
    "        cm = cnf_matrix\n",
    "        title = 'Confusion matrix'\n",
    "    np.set_printoptions(precision=2)\n",
    "    tick_marks = np.arange(0, len(classes_names), 1)\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel('True label')\n",
    "    ax.set_xlabel('Predicted label')\n",
    "    ax.set_xticks(tick_marks)\n",
    "    ax.set_yticks(tick_marks)\n",
    "    ax.set_xticklabels(classes_names)\n",
    "    ax.set_yticklabels(classes_names)\n",
    "    ax.imshow(cm, interpolation='nearest', cmap=clrs.LinearSegmentedColormap.from_list(\"myCMPBlues\", [my_palette['pale blue'], my_palette['blue2'], my_palette['blue3'], my_palette['dark blue']]))\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        ax.text(j, i, format(cm[i, j], fmt), color='w', horizontalalignment=\"center\")\n",
    "\n",
    "\n",
    "def plot_evaluation_results(labels: np.ndarray, trn_y, prd_trn, tst_y, prd_tst):\n",
    "    cnf_mtx_trn = metrics.confusion_matrix(trn_y, prd_trn, labels)\n",
    "    tn_trn, fp_trn, fn_trn, tp_trn = cnf_mtx_trn.ravel()\n",
    "    cnf_mtx_tst = metrics.confusion_matrix(tst_y, prd_tst, labels)\n",
    "    tn_tst, fp_tst, fn_tst, tp_tst = cnf_mtx_tst.ravel()\n",
    "\n",
    "    evaluation = {'Accuracy': [(tn_trn + tp_trn) / (tn_trn + tp_trn + fp_trn + fn_trn),\n",
    "                               (tn_tst + tp_tst) / (tn_tst + tp_tst + fp_tst + fn_tst)],\n",
    "                  'Recall': [tp_trn / (tp_trn + fn_trn), tp_tst / (tp_tst + fn_tst)],\n",
    "                  'Specificity': [tn_trn / (tn_trn + fp_trn), tn_tst / (tn_tst + fp_tst)],\n",
    "                  'Precision': [tp_trn / (tp_trn + fp_trn), tp_tst / (tp_tst + fp_tst)]}\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(2 * HEIGHT, HEIGHT))\n",
    "    multiple_bar_chart(['Train', 'Test'], evaluation, ax=axs[0], title=\"Model's performance over Train and Test sets\")\n",
    "    plot_confusion_matrix(cnf_mtx_tst, labels, ax=axs[1])\n",
    "\n",
    "\n",
    "def plot_roc_chart(models: dict, tstX: np.ndarray, tstY: np.ndarray, ax: plt.Axes = None, target: str = 'class'):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    ax.set_xlim(0.0, 1.0)\n",
    "    ax.set_ylim(0.0, 1.0)\n",
    "    ax.set_xlabel('FP rate')\n",
    "    ax.set_ylabel('TP rate')\n",
    "    ax.set_title('ROC chart for %s' % target)\n",
    "\n",
    "    ax.plot([0, 1], [0, 1], color='navy', label='random', linewidth=1, linestyle='--',  marker='')\n",
    "    for clf in models.keys():\n",
    "        metrics.plot_roc_curve(models[clf], tstX, tstY, ax=ax, marker='', linewidth=1)\n",
    "    ax.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.2</th>\n",
       "      <th>0.3</th>\n",
       "      <th>0.4</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.6</th>\n",
       "      <th>0.7</th>\n",
       "      <th>0.8</th>\n",
       "      <th>0.9</th>\n",
       "      <th>...</th>\n",
       "      <th>0.962</th>\n",
       "      <th>0.963</th>\n",
       "      <th>0.964</th>\n",
       "      <th>0.965</th>\n",
       "      <th>0.966</th>\n",
       "      <th>0.967</th>\n",
       "      <th>0.968</th>\n",
       "      <th>0.969</th>\n",
       "      <th>0.970</th>\n",
       "      <th>new_negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8991.000000</td>\n",
       "      <td>8991.000000</td>\n",
       "      <td>8991.000000</td>\n",
       "      <td>8991.000000</td>\n",
       "      <td>8991.000000</td>\n",
       "      <td>8991.000000</td>\n",
       "      <td>8991.000000</td>\n",
       "      <td>8991.000000</td>\n",
       "      <td>8991.000000</td>\n",
       "      <td>8991.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>8991.000000</td>\n",
       "      <td>8991.000000</td>\n",
       "      <td>8991.000000</td>\n",
       "      <td>8991.000000</td>\n",
       "      <td>8991.000000</td>\n",
       "      <td>8991.000000</td>\n",
       "      <td>8991.000000</td>\n",
       "      <td>8991.000000</td>\n",
       "      <td>8991.000000</td>\n",
       "      <td>8991.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.212991</td>\n",
       "      <td>0.035702</td>\n",
       "      <td>0.068735</td>\n",
       "      <td>0.033367</td>\n",
       "      <td>0.018685</td>\n",
       "      <td>0.144811</td>\n",
       "      <td>0.123123</td>\n",
       "      <td>0.193749</td>\n",
       "      <td>0.013792</td>\n",
       "      <td>0.017351</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061284</td>\n",
       "      <td>0.016683</td>\n",
       "      <td>0.164387</td>\n",
       "      <td>0.239017</td>\n",
       "      <td>0.022467</td>\n",
       "      <td>0.022022</td>\n",
       "      <td>0.021800</td>\n",
       "      <td>0.022356</td>\n",
       "      <td>0.018463</td>\n",
       "      <td>0.917584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.409444</td>\n",
       "      <td>0.185557</td>\n",
       "      <td>0.253018</td>\n",
       "      <td>0.179602</td>\n",
       "      <td>0.135419</td>\n",
       "      <td>0.351930</td>\n",
       "      <td>0.328597</td>\n",
       "      <td>0.395257</td>\n",
       "      <td>0.116631</td>\n",
       "      <td>0.130582</td>\n",
       "      <td>...</td>\n",
       "      <td>0.239863</td>\n",
       "      <td>0.128089</td>\n",
       "      <td>0.370647</td>\n",
       "      <td>0.426507</td>\n",
       "      <td>0.148205</td>\n",
       "      <td>0.146763</td>\n",
       "      <td>0.146037</td>\n",
       "      <td>0.147846</td>\n",
       "      <td>0.134626</td>\n",
       "      <td>0.275012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 1025 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0          0.1          0.2          0.3          0.4  \\\n",
       "count  8991.000000  8991.000000  8991.000000  8991.000000  8991.000000   \n",
       "mean      0.212991     0.035702     0.068735     0.033367     0.018685   \n",
       "std       0.409444     0.185557     0.253018     0.179602     0.135419   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "               0.5          0.6          0.7          0.8          0.9  ...  \\\n",
       "count  8991.000000  8991.000000  8991.000000  8991.000000  8991.000000  ...   \n",
       "mean      0.144811     0.123123     0.193749     0.013792     0.017351  ...   \n",
       "std       0.351930     0.328597     0.395257     0.116631     0.130582  ...   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000  ...   \n",
       "\n",
       "             0.962        0.963        0.964        0.965        0.966  \\\n",
       "count  8991.000000  8991.000000  8991.000000  8991.000000  8991.000000   \n",
       "mean      0.061284     0.016683     0.164387     0.239017     0.022467   \n",
       "std       0.239863     0.128089     0.370647     0.426507     0.148205   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "             0.967        0.968        0.969        0.970  new_negative  \n",
       "count  8991.000000  8991.000000  8991.000000  8991.000000   8991.000000  \n",
       "mean      0.022022     0.021800     0.022356     0.018463      0.917584  \n",
       "std       0.146763     0.146037     0.147846     0.134626      0.275012  \n",
       "min       0.000000     0.000000     0.000000     0.000000      0.000000  \n",
       "25%       0.000000     0.000000     0.000000     0.000000      1.000000  \n",
       "50%       0.000000     0.000000     0.000000     0.000000      1.000000  \n",
       "75%       0.000000     0.000000     0.000000     0.000000      1.000000  \n",
       "max       1.000000     1.000000     1.000000     1.000000      1.000000  \n",
       "\n",
       "[8 rows x 1025 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8991, 1025)\n",
      "(8991, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(original.shape)\n",
    "y: np.ndarray = original.pop('new_negative').values\n",
    "X: np.ndarray = original.values\n",
    "print(X.shape)\n",
    "\n",
    "copy2 = original.copy()\n",
    "copy2['new_negative']=y\n",
    "copy2.head()\n",
    "\n",
    "labels = pd.unique(y)\n",
    "trnX, tstX, trnY, tstY = train_test_split(X, y, train_size=0.7, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11548, 1024)\n"
     ]
    }
   ],
   "source": [
    "sm = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "trnX_min, trnY_min = sm.fit_sample(trnX, trnY)\n",
    "trnX_min.shape\n",
    "print(trnX_min.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9238, 1024)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm = SMOTE(sampling_strategy=0.6, random_state=42)\n",
    "trnX_06, trnY_06 = sm.fit_sample(trnX, trnY)\n",
    "trnX_06.shape\n",
    "#Experimentar também com undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6928, 1024)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler()\n",
    "X_rus, y_rus = rus.fit_sample(trnX_06, trnY_06)\n",
    "X_rus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.1\n",
      "Accuracy score (training): 0.835\n",
      "Accuracy score (validation): 0.823\n",
      "Learning rate:  0.2\n",
      "Accuracy score (training): 0.893\n",
      "Accuracy score (validation): 0.893\n",
      "Learning rate:  0.3\n",
      "Accuracy score (training): 0.884\n",
      "Accuracy score (validation): 0.884\n",
      "Learning rate:  0.4\n",
      "Accuracy score (training): 0.882\n",
      "Accuracy score (validation): 0.878\n",
      "Learning rate:  0.5\n",
      "Accuracy score (training): 0.886\n",
      "Accuracy score (validation): 0.891\n",
      "Learning rate:  0.6\n",
      "Accuracy score (training): 0.878\n",
      "Accuracy score (validation): 0.872\n",
      "Learning rate:  0.7\n",
      "Accuracy score (training): 0.880\n",
      "Accuracy score (validation): 0.874\n",
      "Learning rate:  0.8\n",
      "Accuracy score (training): 0.871\n",
      "Accuracy score (validation): 0.859\n",
      "Learning rate:  0.9\n",
      "Accuracy score (training): 0.891\n",
      "Accuracy score (validation): 0.881\n",
      "Learning rate:  1\n",
      "Accuracy score (training): 0.882\n",
      "Accuracy score (validation): 0.877\n",
      "Learning rate:  0.1\n",
      "Accuracy score (training): 0.868\n",
      "Accuracy score (validation): 0.862\n",
      "Learning rate:  0.2\n",
      "Accuracy score (training): 0.891\n",
      "Accuracy score (validation): 0.891\n",
      "Learning rate:  0.3\n",
      "Accuracy score (training): 0.898\n",
      "Accuracy score (validation): 0.893\n",
      "Learning rate:  0.4\n",
      "Accuracy score (training): 0.907\n",
      "Accuracy score (validation): 0.905\n",
      "Learning rate:  0.5\n",
      "Accuracy score (training): 0.908\n",
      "Accuracy score (validation): 0.911\n",
      "Learning rate:  0.6\n",
      "Accuracy score (training): 0.908\n",
      "Accuracy score (validation): 0.901\n",
      "Learning rate:  0.7\n",
      "Accuracy score (training): 0.915\n",
      "Accuracy score (validation): 0.906\n",
      "Learning rate:  0.8\n",
      "Accuracy score (training): 0.911\n",
      "Accuracy score (validation): 0.899\n",
      "Learning rate:  0.9\n",
      "Accuracy score (training): 0.915\n",
      "Accuracy score (validation): 0.896\n",
      "Learning rate:  1\n",
      "Accuracy score (training): 0.916\n",
      "Accuracy score (validation): 0.899\n",
      "Learning rate:  0.1\n",
      "Accuracy score (training): 0.896\n",
      "Accuracy score (validation): 0.897\n",
      "Learning rate:  0.2\n",
      "Accuracy score (training): 0.921\n",
      "Accuracy score (validation): 0.913\n",
      "Learning rate:  0.3\n",
      "Accuracy score (training): 0.932\n",
      "Accuracy score (validation): 0.921\n",
      "Learning rate:  0.4\n",
      "Accuracy score (training): 0.934\n",
      "Accuracy score (validation): 0.922\n",
      "Learning rate:  0.5\n",
      "Accuracy score (training): 0.942\n",
      "Accuracy score (validation): 0.925\n",
      "Learning rate:  0.6\n",
      "Accuracy score (training): 0.947\n",
      "Accuracy score (validation): 0.925\n",
      "Learning rate:  0.7\n",
      "Accuracy score (training): 0.949\n",
      "Accuracy score (validation): 0.923\n",
      "Learning rate:  0.8\n",
      "Accuracy score (training): 0.949\n",
      "Accuracy score (validation): 0.923\n",
      "Learning rate:  0.9\n",
      "Accuracy score (training): 0.946\n",
      "Accuracy score (validation): 0.918\n",
      "Learning rate:  1\n",
      "Accuracy score (training): 0.945\n",
      "Accuracy score (validation): 0.916\n",
      "Learning rate:  0.1\n",
      "Accuracy score (training): 0.919\n",
      "Accuracy score (validation): 0.916\n",
      "Learning rate:  0.2\n",
      "Accuracy score (training): 0.941\n",
      "Accuracy score (validation): 0.931\n",
      "Learning rate:  0.3\n",
      "Accuracy score (training): 0.949\n",
      "Accuracy score (validation): 0.933\n",
      "Learning rate:  0.4\n",
      "Accuracy score (training): 0.954\n",
      "Accuracy score (validation): 0.933\n",
      "Learning rate:  0.5\n",
      "Accuracy score (training): 0.956\n",
      "Accuracy score (validation): 0.928\n",
      "Learning rate:  0.6\n",
      "Accuracy score (training): 0.964\n",
      "Accuracy score (validation): 0.931\n",
      "Learning rate:  0.7\n",
      "Accuracy score (training): 0.963\n",
      "Accuracy score (validation): 0.928\n",
      "Learning rate:  0.8\n",
      "Accuracy score (training): 0.966\n",
      "Accuracy score (validation): 0.922\n",
      "Learning rate:  0.9\n",
      "Accuracy score (training): 0.965\n",
      "Accuracy score (validation): 0.924\n",
      "Learning rate:  1\n",
      "Accuracy score (training): 0.970\n",
      "Accuracy score (validation): 0.922\n",
      "Learning rate:  0.1\n",
      "Accuracy score (training): 0.932\n",
      "Accuracy score (validation): 0.924\n",
      "Learning rate:  0.2\n",
      "Accuracy score (training): 0.949\n",
      "Accuracy score (validation): 0.931\n",
      "Learning rate:  0.3\n",
      "Accuracy score (training): 0.961\n",
      "Accuracy score (validation): 0.935\n",
      "Learning rate:  0.4\n",
      "Accuracy score (training): 0.965\n",
      "Accuracy score (validation): 0.932\n",
      "Learning rate:  0.5\n",
      "Accuracy score (training): 0.970\n",
      "Accuracy score (validation): 0.935\n",
      "Learning rate:  0.6\n",
      "Accuracy score (training): 0.977\n",
      "Accuracy score (validation): 0.933\n",
      "Learning rate:  0.7\n",
      "Accuracy score (training): 0.977\n",
      "Accuracy score (validation): 0.925\n",
      "Learning rate:  0.8\n",
      "Accuracy score (training): 0.979\n",
      "Accuracy score (validation): 0.929\n",
      "Learning rate:  0.9\n",
      "Accuracy score (training): 0.979\n",
      "Accuracy score (validation): 0.926\n",
      "Learning rate:  1\n",
      "Accuracy score (training): 0.984\n",
      "Accuracy score (validation): 0.918\n",
      "Learning rate:  0.1\n",
      "Accuracy score (training): 0.939\n",
      "Accuracy score (validation): 0.930\n",
      "Learning rate:  0.2\n",
      "Accuracy score (training): 0.957\n",
      "Accuracy score (validation): 0.934\n",
      "Learning rate:  0.3\n",
      "Accuracy score (training): 0.965\n",
      "Accuracy score (validation): 0.937\n",
      "Learning rate:  0.4\n",
      "Accuracy score (training): 0.975\n",
      "Accuracy score (validation): 0.931\n",
      "Learning rate:  0.5\n",
      "Accuracy score (training): 0.979\n",
      "Accuracy score (validation): 0.936\n",
      "Learning rate:  0.6\n",
      "Accuracy score (training): 0.984\n",
      "Accuracy score (validation): 0.929\n",
      "Learning rate:  0.7\n",
      "Accuracy score (training): 0.987\n",
      "Accuracy score (validation): 0.923\n",
      "Learning rate:  0.8\n",
      "Accuracy score (training): 0.988\n",
      "Accuracy score (validation): 0.927\n",
      "Learning rate:  0.9\n",
      "Accuracy score (training): 0.987\n",
      "Accuracy score (validation): 0.923\n",
      "Learning rate:  1\n",
      "Accuracy score (training): 0.993\n",
      "Accuracy score (validation): 0.920\n",
      "Learning rate:  0.1\n",
      "Accuracy score (training): 0.949\n",
      "Accuracy score (validation): 0.933\n",
      "Learning rate:  0.2\n",
      "Accuracy score (training): 0.966\n",
      "Accuracy score (validation): 0.936\n",
      "Learning rate:  0.3\n",
      "Accuracy score (training): 0.980\n",
      "Accuracy score (validation): 0.940\n",
      "Learning rate:  0.4\n",
      "Accuracy score (training): 0.987\n",
      "Accuracy score (validation): 0.935\n",
      "Learning rate:  0.5\n",
      "Accuracy score (training): 0.993\n",
      "Accuracy score (validation): 0.935\n",
      "Learning rate:  0.6\n",
      "Accuracy score (training): 0.992\n",
      "Accuracy score (validation): 0.931\n",
      "Learning rate:  0.7\n",
      "Accuracy score (training): 0.992\n",
      "Accuracy score (validation): 0.924\n",
      "Learning rate:  0.8\n",
      "Accuracy score (training): 0.997\n",
      "Accuracy score (validation): 0.928\n",
      "Learning rate:  0.9\n",
      "Accuracy score (training): 0.992\n",
      "Accuracy score (validation): 0.922\n",
      "Learning rate:  1\n",
      "Accuracy score (training): 0.999\n",
      "Accuracy score (validation): 0.924\n",
      "Learning rate:  0.1\n",
      "Accuracy score (training): 0.956\n",
      "Accuracy score (validation): 0.936\n",
      "Learning rate:  0.2\n",
      "Accuracy score (training): 0.976\n",
      "Accuracy score (validation): 0.938\n",
      "Learning rate:  0.3\n",
      "Accuracy score (training): 0.989\n",
      "Accuracy score (validation): 0.941\n",
      "Learning rate:  0.4\n",
      "Accuracy score (training): 0.994\n",
      "Accuracy score (validation): 0.935\n",
      "Learning rate:  0.5\n",
      "Accuracy score (training): 0.997\n",
      "Accuracy score (validation): 0.936\n",
      "Learning rate:  0.6\n",
      "Accuracy score (training): 0.995\n",
      "Accuracy score (validation): 0.930\n",
      "Learning rate:  0.7\n",
      "Accuracy score (training): 0.994\n",
      "Accuracy score (validation): 0.927\n",
      "Learning rate:  0.8\n",
      "Accuracy score (training): 0.999\n",
      "Accuracy score (validation): 0.930\n",
      "Learning rate:  0.9\n",
      "Accuracy score (training): 0.995\n",
      "Accuracy score (validation): 0.923\n",
      "Learning rate:  1\n",
      "Accuracy score (training): 1.000\n",
      "Accuracy score (validation): 0.924\n",
      "Learning rate:  0.1\n",
      "Accuracy score (training): 0.959\n",
      "Accuracy score (validation): 0.936\n",
      "Learning rate:  0.2\n",
      "Accuracy score (training): 0.982\n",
      "Accuracy score (validation): 0.939\n",
      "Learning rate:  0.3\n",
      "Accuracy score (training): 0.994\n",
      "Accuracy score (validation): 0.943\n",
      "Learning rate:  0.4\n",
      "Accuracy score (training): 0.996\n",
      "Accuracy score (validation): 0.935\n",
      "Learning rate:  0.5\n",
      "Accuracy score (training): 0.999\n",
      "Accuracy score (validation): 0.936\n",
      "Learning rate:  0.6\n",
      "Accuracy score (training): 0.997\n",
      "Accuracy score (validation): 0.930\n",
      "Learning rate:  0.7\n",
      "Accuracy score (training): 0.995\n",
      "Accuracy score (validation): 0.925\n",
      "Learning rate:  0.8\n",
      "Accuracy score (training): 0.999\n",
      "Accuracy score (validation): 0.928\n",
      "Learning rate:  0.9\n",
      "Accuracy score (training): 0.997\n",
      "Accuracy score (validation): 0.921\n",
      "Learning rate:  1\n",
      "Accuracy score (training): 1.000\n",
      "Accuracy score (validation): 0.927\n",
      "Best maximum estimator of 250\n",
      "Best learning rate of 0.3\n",
      "Best cv score of 0.9432913269088213\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "n_estimators = [5, 10, 25, 50, 75, 100, 150, 200, 250]\n",
    "maxestimator = 0\n",
    "maxlearnfraction = 0\n",
    "maxscore = 0\n",
    "rates = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "\n",
    "for i in n_estimators:\n",
    "    for j in rates:\n",
    "        gb_clf = GradientBoostingClassifier(n_estimators = i, learning_rate = j, random_state = 42)\n",
    "        gb_clf.fit(trnX_min, trnY_min)\n",
    "        print(\"Learning rate: \", j)\n",
    "        score_t = gb_clf.score(trnX, trnY)\n",
    "        print(\"Accuracy score (training): {0:.3f}\".format(score_t))\n",
    "        score = gb_clf.score(tstX, tstY)\n",
    "        print(\"Accuracy score (validation): {0:.3f}\".format(score))\n",
    "        maxscore = max(maxscore, score)\n",
    "        if (maxscore == score):\n",
    "            maxestimator = i\n",
    "            maxlearnfraction = j\n",
    "            \n",
    "print(\"Best maximum estimator of\", maxestimator)\n",
    "print(\"Best learning rate of\", maxlearnfraction)\n",
    "print(\"Best cv score of\", maxscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.8773165307635286\n",
      "Logistic Regression f1-score  : 0.9309\n",
      "Logistic Regression precision : 0.9641\n",
      "Logistic Regression recall    : 0.8998\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.63      0.46       222\n",
      "           1       0.96      0.90      0.93      2476\n",
      "\n",
      "    accuracy                           0.88      2698\n",
      "   macro avg       0.66      0.76      0.69      2698\n",
      "weighted avg       0.91      0.88      0.89      2698\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aid/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "log = LogisticRegression(random_state = 42)\n",
    "log.fit(trnX_min, trnY_min)\n",
    "predY = log.predict(tstX)\n",
    "\n",
    "print('Accuracy of logistic regression classifier on test set: {}'.format(log.score(tstX, tstY)))\n",
    "print('Logistic Regression f1-score  : {:.4f}'.format(f1_score(tstY, predY)))\n",
    "print('Logistic Regression precision : {:.4f}'.format(precision_score(tstY, predY)))\n",
    "print('Logistic Regression recall    : {:.4f}'.format(recall_score(tstY, predY)))\n",
    "print(\"\\n\",classification_report(tstY, predY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.1\n",
      "Accuracy score (training): 0.810\n",
      "Accuracy score (validation): 0.812\n",
      "Learning rate:  0.2\n",
      "Accuracy score (training): 0.849\n",
      "Accuracy score (validation): 0.874\n",
      "Learning rate:  0.3\n",
      "Accuracy score (training): 0.854\n",
      "Accuracy score (validation): 0.887\n",
      "Learning rate:  0.4\n",
      "Accuracy score (training): 0.887\n",
      "Accuracy score (validation): 0.880\n",
      "Learning rate:  0.5\n",
      "Accuracy score (training): 0.875\n",
      "Accuracy score (validation): 0.872\n",
      "Learning rate:  0.6\n",
      "Accuracy score (training): 0.877\n",
      "Accuracy score (validation): 0.865\n",
      "Learning rate:  0.7\n",
      "Accuracy score (training): 0.877\n",
      "Accuracy score (validation): 0.866\n",
      "Learning rate:  0.8\n",
      "Accuracy score (training): 0.883\n",
      "Accuracy score (validation): 0.867\n",
      "Learning rate:  0.9\n",
      "Accuracy score (training): 0.887\n",
      "Accuracy score (validation): 0.884\n",
      "Learning rate:  1\n",
      "Accuracy score (training): 0.884\n",
      "Accuracy score (validation): 0.875\n",
      "Learning rate:  0.1\n",
      "Accuracy score (training): 0.845\n",
      "Accuracy score (validation): 0.871\n",
      "Learning rate:  0.2\n",
      "Accuracy score (training): 0.887\n",
      "Accuracy score (validation): 0.887\n",
      "Learning rate:  0.3\n",
      "Accuracy score (training): 0.898\n",
      "Accuracy score (validation): 0.898\n",
      "Learning rate:  0.4\n",
      "Accuracy score (training): 0.910\n",
      "Accuracy score (validation): 0.906\n",
      "Learning rate:  0.5\n",
      "Accuracy score (training): 0.912\n",
      "Accuracy score (validation): 0.901\n",
      "Learning rate:  0.6\n",
      "Accuracy score (training): 0.922\n",
      "Accuracy score (validation): 0.900\n",
      "Learning rate:  0.7\n",
      "Accuracy score (training): 0.922\n",
      "Accuracy score (validation): 0.897\n",
      "Learning rate:  0.8\n",
      "Accuracy score (training): 0.925\n",
      "Accuracy score (validation): 0.888\n",
      "Learning rate:  0.9\n",
      "Accuracy score (training): 0.925\n",
      "Accuracy score (validation): 0.887\n",
      "Learning rate:  1\n",
      "Accuracy score (training): 0.929\n",
      "Accuracy score (validation): 0.887\n",
      "Learning rate:  0.1\n",
      "Accuracy score (training): 0.895\n",
      "Accuracy score (validation): 0.897\n",
      "Learning rate:  0.2\n",
      "Accuracy score (training): 0.925\n",
      "Accuracy score (validation): 0.913\n",
      "Learning rate:  0.3\n",
      "Accuracy score (training): 0.938\n",
      "Accuracy score (validation): 0.917\n",
      "Learning rate:  0.4\n",
      "Accuracy score (training): 0.945\n",
      "Accuracy score (validation): 0.920\n",
      "Learning rate:  0.5\n",
      "Accuracy score (training): 0.952\n",
      "Accuracy score (validation): 0.918\n",
      "Learning rate:  0.6\n",
      "Accuracy score (training): 0.954\n",
      "Accuracy score (validation): 0.911\n",
      "Learning rate:  0.7\n",
      "Accuracy score (training): 0.955\n",
      "Accuracy score (validation): 0.908\n",
      "Learning rate:  0.8\n",
      "Accuracy score (training): 0.962\n",
      "Accuracy score (validation): 0.901\n",
      "Learning rate:  0.9\n",
      "Accuracy score (training): 0.965\n",
      "Accuracy score (validation): 0.906\n",
      "Learning rate:  1\n",
      "Accuracy score (training): 0.964\n",
      "Accuracy score (validation): 0.905\n",
      "Learning rate:  0.1\n",
      "Accuracy score (training): 0.925\n",
      "Accuracy score (validation): 0.913\n",
      "Learning rate:  0.2\n",
      "Accuracy score (training): 0.947\n",
      "Accuracy score (validation): 0.923\n",
      "Learning rate:  0.3\n",
      "Accuracy score (training): 0.959\n",
      "Accuracy score (validation): 0.923\n",
      "Learning rate:  0.4\n",
      "Accuracy score (training): 0.967\n",
      "Accuracy score (validation): 0.927\n",
      "Learning rate:  0.5\n",
      "Accuracy score (training): 0.973\n",
      "Accuracy score (validation): 0.928\n",
      "Learning rate:  0.6\n",
      "Accuracy score (training): 0.976\n",
      "Accuracy score (validation): 0.920\n",
      "Learning rate:  0.7\n",
      "Accuracy score (training): 0.976\n",
      "Accuracy score (validation): 0.914\n",
      "Learning rate:  0.8\n",
      "Accuracy score (training): 0.978\n",
      "Accuracy score (validation): 0.913\n",
      "Learning rate:  0.9\n",
      "Accuracy score (training): 0.981\n",
      "Accuracy score (validation): 0.914\n",
      "Learning rate:  1\n",
      "Accuracy score (training): 0.981\n",
      "Accuracy score (validation): 0.908\n",
      "Learning rate:  0.1\n",
      "Accuracy score (training): 0.941\n",
      "Accuracy score (validation): 0.920\n",
      "Learning rate:  0.2\n",
      "Accuracy score (training): 0.961\n",
      "Accuracy score (validation): 0.930\n",
      "Learning rate:  0.3\n",
      "Accuracy score (training): 0.974\n",
      "Accuracy score (validation): 0.926\n",
      "Learning rate:  0.4\n",
      "Accuracy score (training): 0.977\n",
      "Accuracy score (validation): 0.926\n",
      "Learning rate:  0.5\n",
      "Accuracy score (training): 0.983\n",
      "Accuracy score (validation): 0.927\n",
      "Learning rate:  0.6\n",
      "Accuracy score (training): 0.988\n",
      "Accuracy score (validation): 0.919\n",
      "Learning rate:  0.7\n",
      "Accuracy score (training): 0.987\n",
      "Accuracy score (validation): 0.915\n",
      "Learning rate:  0.8\n",
      "Accuracy score (training): 0.990\n",
      "Accuracy score (validation): 0.912\n",
      "Learning rate:  0.9\n",
      "Accuracy score (training): 0.991\n",
      "Accuracy score (validation): 0.912\n",
      "Learning rate:  1\n",
      "Accuracy score (training): 0.992\n",
      "Accuracy score (validation): 0.908\n",
      "Learning rate:  0.1\n",
      "Accuracy score (training): 0.949\n",
      "Accuracy score (validation): 0.922\n",
      "Learning rate:  0.2\n",
      "Accuracy score (training): 0.967\n",
      "Accuracy score (validation): 0.932\n",
      "Learning rate:  0.3\n",
      "Accuracy score (training): 0.978\n",
      "Accuracy score (validation): 0.930\n",
      "Learning rate:  0.4\n",
      "Accuracy score (training): 0.986\n",
      "Accuracy score (validation): 0.924\n",
      "Learning rate:  0.5\n",
      "Accuracy score (training): 0.989\n",
      "Accuracy score (validation): 0.927\n",
      "Learning rate:  0.6\n",
      "Accuracy score (training): 0.994\n",
      "Accuracy score (validation): 0.922\n",
      "Learning rate:  0.7\n",
      "Accuracy score (training): 0.994\n",
      "Accuracy score (validation): 0.916\n",
      "Learning rate:  0.8\n",
      "Accuracy score (training): 0.996\n",
      "Accuracy score (validation): 0.914\n",
      "Learning rate:  0.9\n",
      "Accuracy score (training): 0.996\n",
      "Accuracy score (validation): 0.915\n",
      "Learning rate:  1\n",
      "Accuracy score (training): 0.997\n",
      "Accuracy score (validation): 0.913\n",
      "Learning rate:  0.1\n",
      "Accuracy score (training): 0.962\n",
      "Accuracy score (validation): 0.927\n",
      "Learning rate:  0.2\n",
      "Accuracy score (training): 0.977\n",
      "Accuracy score (validation): 0.934\n",
      "Learning rate:  0.3\n",
      "Accuracy score (training): 0.989\n",
      "Accuracy score (validation): 0.931\n",
      "Learning rate:  0.4\n",
      "Accuracy score (training): 0.995\n",
      "Accuracy score (validation): 0.928\n",
      "Learning rate:  0.5\n",
      "Accuracy score (training): 0.997\n",
      "Accuracy score (validation): 0.928\n",
      "Learning rate:  0.6\n",
      "Accuracy score (training): 0.998\n",
      "Accuracy score (validation): 0.923\n",
      "Learning rate:  0.7\n",
      "Accuracy score (training): 0.999\n",
      "Accuracy score (validation): 0.917\n",
      "Learning rate:  0.8\n",
      "Accuracy score (training): 0.999\n",
      "Accuracy score (validation): 0.913\n",
      "Learning rate:  0.9\n",
      "Accuracy score (training): 0.999\n",
      "Accuracy score (validation): 0.920\n",
      "Learning rate:  1\n",
      "Accuracy score (training): 0.999\n",
      "Accuracy score (validation): 0.911\n",
      "Learning rate:  0.1\n",
      "Accuracy score (training): 0.968\n",
      "Accuracy score (validation): 0.930\n",
      "Learning rate:  0.2\n",
      "Accuracy score (training): 0.985\n",
      "Accuracy score (validation): 0.934\n",
      "Learning rate:  0.3\n",
      "Accuracy score (training): 0.994\n",
      "Accuracy score (validation): 0.933\n",
      "Learning rate:  0.4\n",
      "Accuracy score (training): 0.998\n",
      "Accuracy score (validation): 0.929\n",
      "Learning rate:  0.5\n",
      "Accuracy score (training): 0.999\n",
      "Accuracy score (validation): 0.927\n",
      "Learning rate:  0.6\n",
      "Accuracy score (training): 0.999\n",
      "Accuracy score (validation): 0.922\n",
      "Learning rate:  0.7\n",
      "Accuracy score (training): 0.999\n",
      "Accuracy score (validation): 0.919\n",
      "Learning rate:  0.8\n",
      "Accuracy score (training): 1.000\n",
      "Accuracy score (validation): 0.916\n",
      "Learning rate:  0.9\n",
      "Accuracy score (training): 1.000\n",
      "Accuracy score (validation): 0.920\n",
      "Learning rate:  1\n",
      "Accuracy score (training): 1.000\n",
      "Accuracy score (validation): 0.912\n",
      "Learning rate:  0.1\n",
      "Accuracy score (training): 0.973\n",
      "Accuracy score (validation): 0.931\n",
      "Learning rate:  0.2\n",
      "Accuracy score (training): 0.991\n",
      "Accuracy score (validation): 0.934\n",
      "Learning rate:  0.3\n",
      "Accuracy score (training): 0.997\n",
      "Accuracy score (validation): 0.935\n",
      "Learning rate:  0.4\n",
      "Accuracy score (training): 0.999\n",
      "Accuracy score (validation): 0.926\n",
      "Learning rate:  0.5\n",
      "Accuracy score (training): 0.999\n",
      "Accuracy score (validation): 0.930\n",
      "Learning rate:  0.6\n",
      "Accuracy score (training): 0.999\n",
      "Accuracy score (validation): 0.926\n",
      "Learning rate:  0.7\n",
      "Accuracy score (training): 1.000\n",
      "Accuracy score (validation): 0.918\n",
      "Learning rate:  0.8\n",
      "Accuracy score (training): 1.000\n",
      "Accuracy score (validation): 0.920\n",
      "Learning rate:  0.9\n",
      "Accuracy score (training): 1.000\n",
      "Accuracy score (validation): 0.922\n",
      "Learning rate:  1\n",
      "Accuracy score (training): 1.000\n",
      "Accuracy score (validation): 0.915\n",
      "Best maximum estimator of 250\n",
      "Best learning rate of 0.3\n",
      "Best cv score of 0.9351371386212008\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "n_estimators = [5, 10, 25, 50, 75, 100, 150, 200, 250]\n",
    "maxestimator = 0\n",
    "maxlearnfraction = 0\n",
    "maxscore = 0\n",
    "rates = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "\n",
    "for i in n_estimators:\n",
    "    for j in rates:\n",
    "        gb_clf = GradientBoostingClassifier(n_estimators = i, learning_rate = j, random_state = 42)\n",
    "        gb_clf.fit(X_rus, y_rus)\n",
    "        print(\"Learning rate: \", j)\n",
    "        score_t = gb_clf.score(X_rus, y_rus)\n",
    "        print(\"Accuracy score (training): {0:.3f}\".format(score_t))\n",
    "        score = gb_clf.score(tstX, tstY)\n",
    "        print(\"Accuracy score (validation): {0:.3f}\".format(score))\n",
    "        maxscore = max(maxscore, score)\n",
    "        if (maxscore == score):\n",
    "            maxestimator = i\n",
    "            maxlearnfraction = j\n",
    "            \n",
    "print(\"Best maximum estimator of\", maxestimator)\n",
    "print(\"Best learning rate of\", maxlearnfraction)\n",
    "print(\"Best cv score of\", maxscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.8713862120088954\n",
      "Logistic Regression f1-score  : 0.9269\n",
      "Logistic Regression precision : 0.9683\n",
      "Logistic Regression recall    : 0.8889\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.68      0.46       222\n",
      "           1       0.97      0.89      0.93      2476\n",
      "\n",
      "    accuracy                           0.87      2698\n",
      "   macro avg       0.66      0.78      0.70      2698\n",
      "weighted avg       0.92      0.87      0.89      2698\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aid/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "log = LogisticRegression(random_state = 42)\n",
    "log.fit(X_rus, y_rus)\n",
    "predY = log.predict(tstX)\n",
    "\n",
    "print('Accuracy of logistic regression classifier on test set: {}'.format(log.score(tstX, tstY)))\n",
    "print('Logistic Regression f1-score  : {:.4f}'.format(f1_score(tstY, predY)))\n",
    "print('Logistic Regression precision : {:.4f}'.format(precision_score(tstY, predY)))\n",
    "print('Logistic Regression recall    : {:.4f}'.format(recall_score(tstY, predY)))\n",
    "print(\"\\n\",classification_report(tstY, predY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
